#!/usr/bin/env python3
import json
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any


def main():
    print("=== Starting data processor ===")
    
    # –ü—É—Ç–∏
    script_dir = Path(__file__).parent
    repo_root = script_dir.parent
    data_path = repo_root / 'data/fio/librawstor'
    output_path = repo_root / 'docs/fio/librawstor/dashboard/data.json'
    
    print(f"Data path: {data_path}")
    print(f"Output path: {output_path}")
    
    if not data_path.exists():
        print("‚ùå Data path does not exist!")
        return
    
    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –¥–∞—Ç—É 30 –¥–Ω–µ–π –Ω–∞–∑–∞–¥
    cutoff_date = datetime.now() - timedelta(days=30)
    print(f"üìÖ Filtering data since: {cutoff_date.strftime('%Y-%m-%d')}")
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    try:
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        all_test_results = collect_raw_data(data_path, cutoff_date)
        print(f"‚úÖ Collected {len(all_test_results)} test results from last 30 days")
        
        if not all_test_results:
            print("‚ö†Ô∏è No data found for the last 30 days. Using all available data.")
            all_test_results = collect_raw_data(data_path, None)
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
        charts_data = prepare_charts_data(all_test_results)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–π JSON
        output = {
            "generated_at": datetime.utcnow().isoformat() + "Z",
            "filter": {
                "days": 30,
                "cutoff_date": cutoff_date.isoformat() + "Z",
                "applied": len(all_test_results) > 0
            },
            "charts": charts_data,
            "summary": get_summary(all_test_results, charts_data, cutoff_date)
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        with output_path.open('w', encoding='utf-8') as f:
            json.dump(output, f, indent=2, ensure_ascii=False)


        print(f"‚úÖ data.json successfully created")
        print_summary(output['summary'])
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()


def parse_timestamp(timestamp_str: str) -> datetime:
    """–ü–∞—Ä—Å–∏—Ç timestamp –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤"""
    if not timestamp_str:
        return None

    # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    timestamp_str = timestamp_str.strip()

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ñ–æ—Ä–º–∞—Ç —Å –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∑–æ–Ω–æ–π –≤ —Ñ–æ—Ä–º–∞—Ç –±–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∑–æ–Ω—ã
    if '+' in timestamp_str:
        # –§–æ—Ä–º–∞—Ç: 2025-08-18T21:04:40+00:00
        timestamp_str = timestamp_str.split('+')[0]  # –ë–µ—Ä–µ–º —á–∞—Å—Ç—å –¥–æ +
    elif '-' in timestamp_str and 'T' in timestamp_str:
        # –§–æ—Ä–º–∞—Ç —Å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∑–æ–Ω–æ–π: 2025-08-18T21:04:40-05:00
        parts = timestamp_str.rsplit('-', 1)
        if len(parts) == 2 and ':' in parts[1]:
            timestamp_str = parts[0]  # –ë–µ—Ä–µ–º —á–∞—Å—Ç—å –¥–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∑–æ–Ω—ã

    # –£–±–∏—Ä–∞–µ–º Z –µ—Å–ª–∏ –µ—Å—Ç—å
    timestamp_str = timestamp_str.rstrip('Z')

    # –ü–æ–ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –¥–∞—Ç
    formats = [
        '%Y-%m-%dT%H:%M:%S.%f',  # ISO —Å –º–∏–∫—Ä–æ—Å–µ–∫—É–Ω–¥–∞–º–∏
        '%Y-%m-%dT%H:%M:%S',  # ISO –±–µ–∑ –º–∏–∫—Ä–æ—Å–µ–∫—É–Ω–¥
        '%Y-%m-%d %H:%M:%S',  # –ü—Ä–æ—Å—Ç–æ–π —Ñ–æ—Ä–º–∞—Ç
        '%Y-%m-%d',  # –¢–æ–ª—å–∫–æ –¥–∞—Ç–∞
    ]

    for fmt in formats:
        try:
            return datetime.strptime(timestamp_str, fmt)
        except ValueError:
            continue

    print(f"‚ö†Ô∏è Could not parse timestamp: {timestamp_str}")
    return None


def is_recent(timestamp: datetime, cutoff_date: datetime) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ timestamp –Ω–µ —Å—Ç–∞—Ä—à–µ cutoff_date"""
    if not timestamp or not cutoff_date:
        return True  # –ï—Å–ª–∏ –Ω–µ—Ç –¥–∞—Ç—ã, –≤–∫–ª—é—á–∞–µ–º –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    return timestamp >= cutoff_date


def collect_raw_data(data_path: Path, cutoff_date: datetime) -> List[Dict[str, Any]]:
    """–°–æ–±–∏—Ä–∞–µ—Ç —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –≤—Ä–µ–º–µ–Ω–∏"""
    all_results = []
    skipped_old = 0
    skipped_no_date = 0
    
    for config_dir in data_path.iterdir():
        if not config_dir.is_dir():
            continue
            
        config_name = config_dir.name
        print(f"Processing configuration: {config_name}")
        
        for json_file in config_dir.glob('*.json'):
            try:
                commit_sha = json_file.stem
                meta_file = json_file.with_suffix('.meta')
                
                # –ß–∏—Ç–∞–µ–º JSON —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Ç–µ—Å—Ç–∞
                test_data = json.loads(json_file.read_text(encoding='utf-8'))
                
                # –ß–∏—Ç–∞–µ–º META —Ñ–∞–π–ª
                meta_data = {}
                if meta_file.exists():
                    meta_data = json.loads(meta_file.read_text(encoding='utf-8'))
                
                # –ü–∞—Ä—Å–∏–º timestamp
                timestamp_str = meta_data.get('date', '')
                timestamp = parse_timestamp(timestamp_str)
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
                if not is_recent(timestamp, cutoff_date):
                    skipped_old += 1
                    continue
                
                if not timestamp:
                    skipped_no_date += 1
                    # –†–µ—à–∞–µ–º –≤–∫–ª—é—á–∞—Ç—å –ª–∏ —Ç–µ—Å—Ç—ã –±–µ–∑ –¥–∞—Ç—ã
                    # –ü–æ–∫–∞ –≤–∫–ª—é—á–∞–µ–º, –Ω–æ –ø–æ–º–µ—á–∞–µ–º
                    timestamp_str = "Unknown date"
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
                metrics = extract_fio_metrics(test_data)
                
                if metrics:
                    test_result = {
                        'config': config_name,
                        'commit_sha': commit_sha,
                        'timestamp': timestamp.isoformat() + 'Z' if timestamp else timestamp_str,
                        'timestamp_obj': timestamp,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ä–µ–∫—Ç –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
                        'branch': meta_data.get('branch', 'unknown'),
                        'test_url': f"../{config_name}/{commit_sha}",
                        **metrics
                    }
                    all_results.append(test_result)
                    
            except Exception as e:
                print(f"  ‚ùå Error processing {json_file.name}: {e}")
    
    if skipped_old > 0:
        print(f"üìÖ Skipped {skipped_old} tests older than cutoff date")
    if skipped_no_date > 0:
        print(f"‚è∞ Skipped {skipped_no_date} tests without date")
    
    return all_results


def extract_fio_metrics(test_data: Dict) -> Dict[str, float]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ IOPS –∏ Latency –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ FIO"""
    metrics = {}
    
    try:
        if 'jobs' in test_data and len(test_data['jobs']) > 0:
            job = test_data['jobs'][0]
            
            # IOPS metrics
            if 'read' in job and 'iops' in job['read']:
                metrics['iops_read'] = float(job['read']['iops'])
            if 'write' in job and 'iops' in job['write']:
                metrics['iops_write'] = float(job['write']['iops'])
            
            # Latency metrics
            if 'read' in job and 'lat_ns' in job['read'] and 'mean' in job['read']['lat_ns']:
                metrics['latency_read'] = float(job['read']['lat_ns']['mean']) / 1_000_000
            if 'write' in job and 'lat_ns' in job['write'] and 'mean' in job['write']['lat_ns']:
                metrics['latency_write'] = float(job['write']['lat_ns']['mean']) / 1_000_000
            
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –ø—É—Ç–∏ –∫ latency
            if 'latency_read' not in metrics and 'read' in job and 'lat' in job['read'] and 'mean' in job['read']['lat']:
                metrics['latency_read'] = float(job['read']['lat']['mean']) / 1000
            if 'latency_write' not in metrics and 'write' in job and 'lat' in job['write'] and 'mean' in job['write']['lat']:
                metrics['latency_write'] = float(job['write']['lat']['mean']) / 1000
                
    except Exception as e:
        print(f"    ‚ö†Ô∏è Could not extract metrics: {e}")
    
    return metrics


def prepare_charts_data(all_results: List[Dict]) -> Dict[str, List[Dict]]:
    """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è 8 –≥—Ä–∞—Ñ–∏–∫–æ–≤"""
    charts_data = {}
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏
    sorted_results = sorted(
        [r for r in all_results if r.get('timestamp_obj')], 
        key=lambda x: x['timestamp_obj']
    )
    
    # –î–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–µ–∑ –¥–∞—Ç—ã –¥–æ–±–∞–≤–ª—è–µ–º –≤ –∫–æ–Ω–µ—Ü
    no_date_results = [r for r in all_results if not r.get('timestamp_obj')]
    sorted_results.extend(no_date_results)
    
    metrics = ['iops_read', 'iops_write', 'latency_read', 'latency_write']
    
    for metric in metrics:
        charts_data[f"{metric}_by_config"] = group_by_config(sorted_results, metric)
        charts_data[f"{metric}_by_branch"] = group_by_branch(sorted_results, metric)
    
    return charts_data


def group_by_config(all_results: List[Dict], metric: str) -> List[Dict]:
    """–ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø–æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è–º —Ç–µ—Å—Ç–æ–≤"""
    valid_results = [r for r in all_results if metric in r and r[metric] is not None]
    
    config_groups = {}
    for result in valid_results:
        config = result['config']
        if config not in config_groups:
            config_groups[config] = []
        
        config_groups[config].append({
            'group': config,
            'timestamp': result['timestamp'],
            'value': result[metric],
            'commit_sha': result['commit_sha'],
            'branch': result['branch'],
            'test_url': result['test_url'],
            'config': config
        })

    chart_data = []
    for config, points in config_groups.items():
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        sorted_points = sorted(
            points, 
            key=lambda x: x['timestamp'] if x['timestamp'] != "Unknown date" else "9999-12-31"
        )
        chart_data.extend(sorted_points)
    
    return chart_data


def group_by_branch(all_results: List[Dict], metric: str) -> List[Dict]:
    """–ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø–æ –≤–µ—Ç–∫–∞–º"""
    valid_results = [r for r in all_results if metric in r and r[metric] is not None]
    
    branch_groups = {}
    for result in valid_results:
        branch = result['branch']
        if branch not in branch_groups:
            branch_groups[branch] = []
        
        branch_groups[branch].append({
            'group': branch,
            'timestamp': result['timestamp'],
            'value': result[metric],
            'commit_sha': result['commit_sha'],
            'config': result['config'],
            'test_url': result['test_url'],
            'branch': branch
        })
    
    chart_data = []
    for branch, points in branch_groups.items():
        sorted_points = sorted(
            points, 
            key=lambda x: x['timestamp'] if x['timestamp'] != "Unknown date" else "9999-12-31"
        )
        chart_data.extend(sorted_points)
    
    return chart_data


def get_summary(all_results: List[Dict], charts_data: Dict, cutoff_date: datetime) -> Dict[str, Any]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç summary –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é"""
    configs = set(r['config'] for r in all_results)
    branches = set(r['branch'] for r in all_results)
    
    # –í—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω
    timestamps = [r['timestamp_obj'] for r in all_results if r.get('timestamp_obj')]
    time_range = {
        'start': min(timestamps).isoformat() + 'Z' if timestamps else None,
        'end': max(timestamps).isoformat() + 'Z' if timestamps else None
    }
    
    # –¢–µ—Å—Ç—ã –±–µ–∑ –¥–∞—Ç—ã
    tests_without_date = len([r for r in all_results if not r.get('timestamp_obj')])
    
    chart_points = {chart: len(data) for chart, data in charts_data.items()}
    
    return {
        'total_tests': len(all_results),
        'tests_without_date': tests_without_date,
        'total_configurations': len(configs),
        'total_branches': len(branches),
        'configurations': sorted(list(configs)),
        'branches': sorted(list(branches)),
        'time_range': time_range,
        'cutoff_date': cutoff_date.isoformat() + 'Z',
        'chart_points': chart_points
    }


def print_summary(summary: Dict):
    """–ü–µ—á–∞—Ç–∞–µ—Ç –∫—Ä–∞—Å–∏–≤—ã–π summary"""
    print("\n=== SUMMARY ===")
    print(f"Total tests: {summary['total_tests']}")
    if summary['tests_without_date'] > 0:
        print(f"Tests without date: {summary['tests_without_date']}")
    print(f"Configurations: {summary['total_configurations']}")
    print(f"Branches: {summary['total_branches']}")
    
    if summary['time_range']['start']:
        start_date = datetime.fromisoformat(summary['time_range']['start'].replace('Z', ''))
        end_date = datetime.fromisoformat(summary['time_range']['end'].replace('Z', ''))
        cutoff_date = datetime.fromisoformat(summary['cutoff_date'].replace('Z', ''))
        
        print(f"Data range: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}")
        print(f"Cutoff date: {cutoff_date.strftime('%Y-%m-%d')} (-30 days)")
    
    print("\nChart data points:")
    for chart, count in summary['chart_points'].items():
        print(f"  {chart}: {count} points")
    
    print(f"\nConfigurations: {', '.join(summary['configurations'])}")
    print(f"Branches: {', '.join(summary['branches'])}")


if __name__ == "__main__":
    main()
